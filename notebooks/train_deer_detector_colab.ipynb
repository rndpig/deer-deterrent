{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209f32af",
   "metadata": {},
   "source": [
    "# Deer Detection Model Training - Google Colab\n",
    "\n",
    "Train a YOLOv8 deer detection model using Google Colab's free GPU.\n",
    "\n",
    "**Before running:**\n",
    "1. Go to `Runtime` â†’ `Change runtime type`\n",
    "2. Select `T4 GPU` as Hardware accelerator\n",
    "3. Click `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca339594",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48baf48",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "drive_root = Path('/content/drive/MyDrive')\n",
    "project_folder = drive_root / 'Deer video detection'\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"Project folder: {project_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d8bbf",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0df367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths for training data\n",
    "# The train-model endpoint exports data to the root of the Drive folder\n",
    "annotations_file = project_folder / 'annotations.json'\n",
    "images_dir = project_folder / 'images'\n",
    "\n",
    "# Verify data exists\n",
    "if not annotations_file.exists():\n",
    "    print(\"âŒ annotations.json not found!\")\n",
    "    print(f\"   Expected: {annotations_file}\")\n",
    "    print(\"\\nðŸ“ Run the 'Train Model' button in the web app first!\")\n",
    "    raise FileNotFoundError(\"Training data not found\")\n",
    "\n",
    "if not images_dir.exists():\n",
    "    print(\"âŒ images folder not found!\")\n",
    "    print(f\"   Expected: {images_dir}\")\n",
    "    raise FileNotFoundError(\"Images folder not found\")\n",
    "\n",
    "image_count = len(list(images_dir.glob('*.jpg')))\n",
    "print(f\"âœ“ Found {image_count} training images\")\n",
    "print(f\"âœ“ Annotations file: {annotations_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO to YOLO format\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Load COCO annotations\n",
    "print(\"Loading COCO annotations...\")\n",
    "with open(annotations_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "print(f\"  Images: {len(coco_data['images'])}\")\n",
    "print(f\"  Annotations: {len(coco_data['annotations'])}\")\n",
    "print(f\"  Categories: {len(coco_data['categories'])}\")\n",
    "\n",
    "# Create working directory and output directories\n",
    "work_dir = Path('/content/deer-detection')\n",
    "dataset_dir = work_dir / 'dataset'\n",
    "train_images_dir = dataset_dir / 'images' / 'train'\n",
    "train_labels_dir = dataset_dir / 'labels' / 'train'\n",
    "val_images_dir = dataset_dir / 'images' / 'val'\n",
    "val_labels_dir = dataset_dir / 'labels' / 'val'\n",
    "\n",
    "for d in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Image ID to info mapping\n",
    "image_id_to_info = {img['id']: img for img in coco_data['images']}\n",
    "\n",
    "# Image ID to annotations mapping\n",
    "image_id_to_annots = {}\n",
    "for annot in coco_data['annotations']:\n",
    "    image_id = annot['image_id']\n",
    "    if image_id not in image_id_to_annots:\n",
    "        image_id_to_annots[image_id] = []\n",
    "    image_id_to_annots[image_id].append(annot)\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "import random\n",
    "all_image_ids = list(image_id_to_info.keys())\n",
    "random.shuffle(all_image_ids)\n",
    "split_idx = int(len(all_image_ids) * 0.8)\n",
    "train_ids = set(all_image_ids[:split_idx])\n",
    "val_ids = set(all_image_ids[split_idx:])\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(train_ids)} images\")\n",
    "print(f\"  Val: {len(val_ids)} images\")\n",
    "\n",
    "# Convert each image\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "for image_id, image_info in image_id_to_info.items():\n",
    "    # Get image file\n",
    "    image_filename = Path(image_info['file_name']).name\n",
    "    image_path = images_dir / image_filename\n",
    "    \n",
    "    if not image_path.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Get annotations\n",
    "    annotations = image_id_to_annots.get(image_id, [])\n",
    "    if not annotations:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Determine if train or val\n",
    "    if image_id in train_ids:\n",
    "        dest_images_dir = train_images_dir\n",
    "        dest_labels_dir = train_labels_dir\n",
    "    else:\n",
    "        dest_images_dir = val_images_dir\n",
    "        dest_labels_dir = val_labels_dir\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width = image_info['width']\n",
    "    height = image_info['height']\n",
    "    \n",
    "    # Convert to YOLO format\n",
    "    yolo_annotations = []\n",
    "    for annot in annotations:\n",
    "        bbox = annot['bbox']  # [x, y, width, height]\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Convert to YOLO format [class, x_center, y_center, width, height] normalized\n",
    "        x_center = (x + w / 2) / width\n",
    "        y_center = (y + h / 2) / height\n",
    "        norm_width = w / width\n",
    "        norm_height = h / height\n",
    "        \n",
    "        yolo_annotations.append(f\"0 {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\")\n",
    "    \n",
    "    # Copy image\n",
    "    shutil.copy2(image_path, dest_images_dir / image_filename)\n",
    "    \n",
    "    # Save label\n",
    "    label_filename = Path(image_filename).stem + \".txt\"\n",
    "    with open(dest_labels_dir / label_filename, 'w') as f:\n",
    "        f.write('\\n'.join(yolo_annotations))\n",
    "    \n",
    "    processed += 1\n",
    "\n",
    "print(f\"\\nâœ“ Conversion complete!\")\n",
    "print(f\"  Processed: {processed} images\")\n",
    "print(f\"  Skipped: {skipped} images (no annotations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset.yaml\n",
    "dataset_yaml = dataset_dir / 'dataset.yaml'\n",
    "\n",
    "# Use absolute paths for reliability\n",
    "yaml_content = {\n",
    "    'path': str(dataset_dir.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: 'deer'},\n",
    "    'nc': 1\n",
    "}\n",
    "\n",
    "with open(dataset_yaml, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "print(\"âœ“ Created dataset.yaml\")\n",
    "print(f\"  Path: {dataset_yaml}\")\n",
    "print(f\"\\nDataset configuration:\")\n",
    "for key, value in yaml_content.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1e2eb",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected. Training will be slow!\")\n",
    "    print(\"Go to Runtime â†’ Change runtime type â†’ Select T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d11243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = YOLO('yolov8n.pt')  # YOLOv8 nano (smallest/fastest)\n",
    "\n",
    "print(\"âœ“ Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with better error handling\n",
    "try:\n",
    "    # Verify dataset exists\n",
    "    print(f\"Dataset config: {dataset_yaml}\")\n",
    "    print(f\"Dataset exists: {dataset_yaml.exists()}\")\n",
    "    \n",
    "    # Check GPU\n",
    "    device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Start training\n",
    "    results = model.train(\n",
    "        data=str(dataset_yaml),\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=8,  # Reduced from 16 for stability\n",
    "        patience=50,  # Early stopping\n",
    "        save_period=10,  # Save checkpoint every 10 epochs\n",
    "        device=device,\n",
    "        project=str(work_dir / 'runs'),\n",
    "        name='deer_detector',\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "        workers=2  # Reduced for Colab stability\n",
    "    )\n",
    "    print(\"âœ“ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Debug info\n",
    "    print(\"\\nDebug info:\")\n",
    "    print(f\"Dataset dir exists: {dataset_dir.exists()}\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = dataset_dir / 'images' / split\n",
    "        print(f\"{split} exists: {split_dir.exists()}, images: {len(list(split_dir.glob('*.jpg'))) if split_dir.exists() else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25829c",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "metrics = model.val(data=str(dataset_yaml), split='test')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Test Set Results:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"mAP50: {metrics.box.map50:.3f}\")\n",
    "print(f\"mAP50-95: {metrics.box.map:.3f}\")\n",
    "print(f\"Precision: {metrics.box.p[0]:.3f}\")\n",
    "print(f\"Recall: {metrics.box.r[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed4281",
   "metadata": {},
   "source": [
    "## 6. Test on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67864528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "test_images_dir = dataset_dir / 'images/test'\n",
    "test_images = list(test_images_dir.glob('*.jpg'))[:5]  # First 5 test images\n",
    "\n",
    "for img_path in test_images:\n",
    "    results = model(str(img_path))\n",
    "    \n",
    "    # Save annotated image\n",
    "    annotated = results[0].plot()\n",
    "    output_path = work_dir / f\"test_{img_path.name}\"\n",
    "    import cv2\n",
    "    cv2.imwrite(str(output_path), annotated)\n",
    "    \n",
    "    # Display\n",
    "    print(f\"\\n{img_path.name}:\")\n",
    "    display(IPImage(filename=str(output_path), width=600))\n",
    "    \n",
    "    # Print detections\n",
    "    if len(results[0].boxes) > 0:\n",
    "        for box in results[0].boxes:\n",
    "            conf = box.conf[0]\n",
    "            print(f\"  Deer detected - Confidence: {conf:.2f}\")\n",
    "    else:\n",
    "        print(\"  No deer detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84164624",
   "metadata": {},
   "source": [
    "## 7. Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4873761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best model to Google Drive\n",
    "best_model = work_dir / 'runs/deer_detector/weights/best.pt'\n",
    "output_model_dir = project_folder / 'trained_models'\n",
    "output_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_model_path = output_model_dir / 'deer_detector_best.pt'\n",
    "shutil.copy2(best_model, output_model_path)\n",
    "\n",
    "print(f\"âœ“ Model saved to Google Drive: {output_model_path}\")\n",
    "\n",
    "# Also save last checkpoint\n",
    "last_model = work_dir / 'runs/deer_detector/weights/last.pt'\n",
    "if last_model.exists():\n",
    "    shutil.copy2(last_model, output_model_dir / 'deer_detector_last.pt')\n",
    "    print(f\"âœ“ Last checkpoint saved\")\n",
    "\n",
    "# Copy training results\n",
    "results_dir = output_model_dir / 'training_results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for file in ['results.png', 'confusion_matrix.png', 'results.csv']:\n",
    "    src = work_dir / 'runs/deer_detector' / file\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, results_dir / file)\n",
    "\n",
    "print(f\"âœ“ Training results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092a3c2",
   "metadata": {},
   "source": [
    "## 8. Download Model to Local Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading model file...\")\n",
    "files.download(str(best_model))\n",
    "print(\"âœ“ Download complete!\")\n",
    "print(\"\\nSave this file as: models/production/best.pt in your local project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499984b",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download the model** (downloaded above) or access it from Google Drive:\n",
    "   - `Deer video detection/trained_models/deer_detector_best.pt`\n",
    "\n",
    "2. **Copy to your local project**:\n",
    "   ```\n",
    "   models/production/best.pt\n",
    "   ```\n",
    "\n",
    "3. **Test locally**:\n",
    "   ```bash\n",
    "   python src/inference/detector.py\n",
    "   ```\n",
    "\n",
    "4. **Configure Ring & Rainbird** and run the full system:\n",
    "   ```bash\n",
    "   python src/main.py\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
