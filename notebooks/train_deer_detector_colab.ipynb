{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209f32af",
   "metadata": {},
   "source": [
    "# Deer Detection Model Training - Google Colab\n",
    "\n",
    "Train a YOLOv8 deer detection model using Google Colab's free GPU.\n",
    "\n",
    "**Before running:**\n",
    "1. Go to `Runtime` â†’ `Change runtime type`\n",
    "2. Select `T4 GPU` as Hardware accelerator\n",
    "3. Click `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca339594",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48baf48",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "drive_root = Path('/content/drive/MyDrive')\n",
    "project_folder = drive_root / 'Deer video detection'\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"Project folder: {project_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d8bbf",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0df367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directory\n",
    "work_dir = Path('/content/deer-detection')\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract annotations if needed\n",
    "annotations_zip = project_folder / 'videos/annotations/annotations_coco.zip'\n",
    "annotations_dir = project_folder / 'videos/annotations'\n",
    "\n",
    "if not (annotations_dir / 'images').exists() and annotations_zip.exists():\n",
    "    print(\"Extracting annotations...\")\n",
    "    !unzip -q \"{annotations_zip}\" -d \"{annotations_dir}\"\n",
    "    print(\"âœ“ Annotations extracted\")\n",
    "else:\n",
    "    print(\"âœ“ Annotations already available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO to YOLO format\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Load COCO annotations\n",
    "coco_json = annotations_dir / 'result.json'\n",
    "with open(coco_json, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Create output directories\n",
    "dataset_dir = work_dir / 'dataset'\n",
    "images_dir = dataset_dir / 'images'\n",
    "labels_dir = dataset_dir / 'labels'\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Image ID to info mapping\n",
    "image_id_to_info = {img['id']: img for img in coco_data['images']}\n",
    "\n",
    "# Image ID to annotations mapping\n",
    "image_id_to_annots = {}\n",
    "for annot in coco_data['annotations']:\n",
    "    image_id = annot['image_id']\n",
    "    if image_id not in image_id_to_annots:\n",
    "        image_id_to_annots[image_id] = []\n",
    "    image_id_to_annots[image_id].append(annot)\n",
    "\n",
    "# Convert each image\n",
    "processed = 0\n",
    "skipped = 0\n",
    "source_images_dir = annotations_dir / 'images'\n",
    "\n",
    "for image_id, image_info in image_id_to_info.items():\n",
    "    # Get image file\n",
    "    image_filename = Path(image_info['file_name']).name\n",
    "    image_path = source_images_dir / image_filename\n",
    "    \n",
    "    if not image_path.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Get annotations\n",
    "    annotations = image_id_to_annots.get(image_id, [])\n",
    "    if not annotations:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width = image_info['width']\n",
    "    height = image_info['height']\n",
    "    \n",
    "    # Convert to YOLO format\n",
    "    yolo_annotations = []\n",
    "    for annot in annotations:\n",
    "        bbox = annot['bbox']  # [x, y, width, height]\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Convert to YOLO format [class, x_center, y_center, width, height] normalized\n",
    "        x_center = (x + w / 2) / width\n",
    "        y_center = (y + h / 2) / height\n",
    "        norm_width = w / width\n",
    "        norm_height = h / height\n",
    "        \n",
    "        yolo_annotations.append(f\"0 {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\")\n",
    "    \n",
    "    # Copy image\n",
    "    shutil.copy2(image_path, images_dir / image_filename)\n",
    "    \n",
    "    # Save label\n",
    "    label_filename = Path(image_filename).stem + \".txt\"\n",
    "    with open(labels_dir / label_filename, 'w') as f:\n",
    "        f.write('\\n'.join(yolo_annotations))\n",
    "    \n",
    "    processed += 1\n",
    "\n",
    "print(f\"\\nâœ“ Conversion complete!\")\n",
    "print(f\"  Processed: {processed} images\")\n",
    "print(f\"  Skipped: {skipped} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val/test\n",
    "import random\n",
    "\n",
    "# Get all images\n",
    "all_images = list(images_dir.glob('*.jpg'))\n",
    "random.shuffle(all_images)\n",
    "\n",
    "print(f\"Total images found: {len(all_images)}\")\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_count = int(len(all_images) * train_ratio)\n",
    "val_count = int(len(all_images) * val_ratio)\n",
    "\n",
    "train_images = all_images[:train_count]\n",
    "val_images = all_images[train_count:train_count + val_count]\n",
    "test_images = all_images[train_count + val_count:]\n",
    "\n",
    "# Create split directories with correct structure\n",
    "for split in ['train', 'val', 'test']:\n",
    "    (dataset_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "    (dataset_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy files to splits (use copy instead of move for safety)\n",
    "def copy_to_split(images, split_name):\n",
    "    count = 0\n",
    "    for img_path in images:\n",
    "        # Copy image\n",
    "        dst_img = dataset_dir / 'images' / split_name / img_path.name\n",
    "        shutil.copy2(str(img_path), str(dst_img))\n",
    "        \n",
    "        # Copy label\n",
    "        label_name = img_path.stem + '.txt'\n",
    "        label_path = labels_dir / label_name\n",
    "        if label_path.exists():\n",
    "            dst_label = dataset_dir / 'labels' / split_name / label_name\n",
    "            shutil.copy2(str(label_path), str(dst_label))\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "train_copied = copy_to_split(train_images, 'train')\n",
    "val_copied = copy_to_split(val_images, 'val')\n",
    "test_copied = copy_to_split(test_images, 'test')\n",
    "\n",
    "print(f\"\\nâœ“ Dataset split:\")\n",
    "print(f\"  Train: {train_copied} images\")\n",
    "print(f\"  Val:   {val_copied} images\")\n",
    "print(f\"  Test:  {test_copied} images\")\n",
    "\n",
    "# Verify directories exist\n",
    "for split in ['train', 'val', 'test']:\n",
    "    img_path = dataset_dir / 'images' / split\n",
    "    lbl_path = dataset_dir / 'labels' / split\n",
    "    img_count = len(list(img_path.glob('*.jpg')))\n",
    "    lbl_count = len(list(lbl_path.glob('*.txt')))\n",
    "    print(f\"  {split}: {img_count} images, {lbl_count} labels\")\n",
    "    if img_count == 0:\n",
    "        print(f\"  âš ï¸ WARNING: No images in {split} split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset.yaml\n",
    "dataset_yaml = dataset_dir / 'dataset.yaml'\n",
    "\n",
    "# Use absolute paths for reliability\n",
    "yaml_content = {\n",
    "    'path': str(dataset_dir.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'test': 'images/test',\n",
    "    'names': {0: 'deer'},\n",
    "    'nc': 1\n",
    "}\n",
    "\n",
    "with open(dataset_yaml, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ“ Created {dataset_yaml}\")\n",
    "print(f\"\\nDataset configuration:\")\n",
    "with open(dataset_yaml, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Verify all paths exist\n",
    "print(f\"\\nVerifying paths:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    img_dir = dataset_dir / 'images' / split\n",
    "    exists = img_dir.exists()\n",
    "    count = len(list(img_dir.glob('*.jpg'))) if exists else 0\n",
    "    status = \"âœ“\" if exists and count > 0 else \"âŒ\"\n",
    "    print(f\"  {status} {split}: {img_dir} ({count} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1e2eb",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected. Training will be slow!\")\n",
    "    print(\"Go to Runtime â†’ Change runtime type â†’ Select T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d11243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = YOLO('yolov8n.pt')  # YOLOv8 nano (smallest/fastest)\n",
    "\n",
    "print(\"âœ“ Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with better error handling\n",
    "try:\n",
    "    # Verify dataset exists\n",
    "    print(f\"Dataset config: {dataset_yaml}\")\n",
    "    print(f\"Dataset exists: {dataset_yaml.exists()}\")\n",
    "    \n",
    "    # Check GPU\n",
    "    device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Start training\n",
    "    results = model.train(\n",
    "        data=str(dataset_yaml),\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=8,  # Reduced from 16 for stability\n",
    "        patience=50,  # Early stopping\n",
    "        save_period=10,  # Save checkpoint every 10 epochs\n",
    "        device=device,\n",
    "        project=str(work_dir / 'runs'),\n",
    "        name='deer_detector',\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "        workers=2  # Reduced for Colab stability\n",
    "    )\n",
    "    print(\"âœ“ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Debug info\n",
    "    print(\"\\nDebug info:\")\n",
    "    print(f\"Dataset dir exists: {dataset_dir.exists()}\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = dataset_dir / 'images' / split\n",
    "        print(f\"{split} exists: {split_dir.exists()}, images: {len(list(split_dir.glob('*.jpg'))) if split_dir.exists() else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25829c",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "metrics = model.val(data=str(dataset_yaml), split='test')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Test Set Results:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"mAP50: {metrics.box.map50:.3f}\")\n",
    "print(f\"mAP50-95: {metrics.box.map:.3f}\")\n",
    "print(f\"Precision: {metrics.box.p[0]:.3f}\")\n",
    "print(f\"Recall: {metrics.box.r[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed4281",
   "metadata": {},
   "source": [
    "## 6. Test on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67864528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "test_images_dir = dataset_dir / 'images/test'\n",
    "test_images = list(test_images_dir.glob('*.jpg'))[:5]  # First 5 test images\n",
    "\n",
    "for img_path in test_images:\n",
    "    results = model(str(img_path))\n",
    "    \n",
    "    # Save annotated image\n",
    "    annotated = results[0].plot()\n",
    "    output_path = work_dir / f\"test_{img_path.name}\"\n",
    "    import cv2\n",
    "    cv2.imwrite(str(output_path), annotated)\n",
    "    \n",
    "    # Display\n",
    "    print(f\"\\n{img_path.name}:\")\n",
    "    display(IPImage(filename=str(output_path), width=600))\n",
    "    \n",
    "    # Print detections\n",
    "    if len(results[0].boxes) > 0:\n",
    "        for box in results[0].boxes:\n",
    "            conf = box.conf[0]\n",
    "            print(f\"  Deer detected - Confidence: {conf:.2f}\")\n",
    "    else:\n",
    "        print(\"  No deer detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84164624",
   "metadata": {},
   "source": [
    "## 7. Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4873761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best model to Google Drive\n",
    "best_model = work_dir / 'runs/deer_detector/weights/best.pt'\n",
    "output_model_dir = project_folder / 'trained_models'\n",
    "output_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_model_path = output_model_dir / 'deer_detector_best.pt'\n",
    "shutil.copy2(best_model, output_model_path)\n",
    "\n",
    "print(f\"âœ“ Model saved to Google Drive: {output_model_path}\")\n",
    "\n",
    "# Also save last checkpoint\n",
    "last_model = work_dir / 'runs/deer_detector/weights/last.pt'\n",
    "if last_model.exists():\n",
    "    shutil.copy2(last_model, output_model_dir / 'deer_detector_last.pt')\n",
    "    print(f\"âœ“ Last checkpoint saved\")\n",
    "\n",
    "# Copy training results\n",
    "results_dir = output_model_dir / 'training_results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for file in ['results.png', 'confusion_matrix.png', 'results.csv']:\n",
    "    src = work_dir / 'runs/deer_detector' / file\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, results_dir / file)\n",
    "\n",
    "print(f\"âœ“ Training results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092a3c2",
   "metadata": {},
   "source": [
    "## 8. Download Model to Local Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading model file...\")\n",
    "files.download(str(best_model))\n",
    "print(\"âœ“ Download complete!\")\n",
    "print(\"\\nSave this file as: models/production/best.pt in your local project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499984b",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download the model** (downloaded above) or access it from Google Drive:\n",
    "   - `Deer video detection/trained_models/deer_detector_best.pt`\n",
    "\n",
    "2. **Copy to your local project**:\n",
    "   ```\n",
    "   models/production/best.pt\n",
    "   ```\n",
    "\n",
    "3. **Test locally**:\n",
    "   ```bash\n",
    "   python src/inference/detector.py\n",
    "   ```\n",
    "\n",
    "4. **Configure Ring & Rainbird** and run the full system:\n",
    "   ```bash\n",
    "   python src/main.py\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
