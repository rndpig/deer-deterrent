{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23f0516",
   "metadata": {},
   "source": [
    "# ðŸ¦Œ Deer Detector v2.0 â€” YOLO Training Notebook\n",
    "\n",
    "**Project:** Deer Deterrent System  \n",
    "**Model:** YOLO26s (real YOLO26) or YOLOv8s (fallback)  \n",
    "**Dataset:** v2.0 with CLAHE preprocessing + negatives  \n",
    "**Hardware:** Colab T4 GPU  \n",
    "\n",
    "## Steps:\n",
    "1. Setup environment\n",
    "2. Upload dataset\n",
    "3. Phase 1: Frozen backbone training (20 epochs)\n",
    "4. Phase 2: Full fine-tune (130 epochs)\n",
    "5. Evaluate on test set\n",
    "6. Export for deployment (ONNX + OpenVINO)\n",
    "7. Download trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f388e",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3aa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics>=8.4.0 opencv-python-headless\n",
    "\n",
    "# Verify\n",
    "import ultralytics\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdfc27",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n",
    "\n",
    "Upload your `dataset_v2.0.tar.gz` exported by `scripts/export_dataset_v2.py`.\n",
    "\n",
    "**Option A:** Upload directly to Colab (temporary â€” lost when session ends)  \n",
    "**Option B:** Upload to Google Drive first (persistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Direct upload\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create dataset directory\n",
    "DATASET_DIR = \"/content/dataset\"\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Upload dataset_v2.0.tar.gz:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.tar.gz'):\n",
    "        !tar xzf \"{filename}\" -C {DATASET_DIR}\n",
    "        print(f\"Extracted {filename} to {DATASET_DIR}\")\n",
    "    elif filename.endswith('.zip'):\n",
    "        !unzip -q \"{filename}\" -d {DATASET_DIR}\n",
    "        print(f\"Extracted {filename} to {DATASET_DIR}\")\n",
    "\n",
    "# Verify\n",
    "!ls -la {DATASET_DIR}/\n",
    "!cat {DATASET_DIR}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d965704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: From Google Drive (uncomment if using Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# DATASET_DIR = \"/content/dataset\"\n",
    "# os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "# !tar xzf /content/drive/MyDrive/deer-deterrent/dataset_v2.0.tar.gz -C {DATASET_DIR}\n",
    "# !cat {DATASET_DIR}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ce0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_DIR = \"/content/dataset\"\n",
    "ds = Path(DATASET_DIR)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    imgs = list((ds / 'images' / split).glob('*.jpg'))\n",
    "    lbls = list((ds / 'labels' / split).glob('*.txt'))\n",
    "    pos = sum(1 for l in lbls if l.stat().st_size > 0)\n",
    "    neg = len(lbls) - pos\n",
    "    print(f\"  {split:5s}: {len(imgs)} images, {len(lbls)} labels (positive: {pos}, negative: {neg})\")\n",
    "\n",
    "print(f\"\\n  Total: {sum(len(list((ds/'images'/s).glob('*.jpg'))) for s in ['train','val','test'])} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa7391",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Frozen Backbone Training (20 epochs)\n",
    "\n",
    "Freeze the COCO-pretrained backbone and only train the detection head.\n",
    "This lets the head adapt to deer-specific features without destroying\n",
    "the useful low-level feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "DATASET_DIR = \"/content/dataset\"\n",
    "DATA_YAML = f\"{DATASET_DIR}/data.yaml\"\n",
    "OUTPUT_DIR = \"/content/runs\"\n",
    "RUN_NAME = f\"deer_v2_{datetime.now().strftime('%m%d_%H%M')}\"\n",
    "\n",
    "# Try YOLO26s first, fallback to YOLOv8s\n",
    "try:\n",
    "    model = YOLO('yolo26s.pt')\n",
    "    ARCH = 'YOLO26s'\n",
    "    print(f\"âœ… Loaded YOLO26s (real YOLO26)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ YOLO26s not available ({e}), using YOLOv8s\")\n",
    "    model = YOLO('yolov8s.pt')\n",
    "    ARCH = 'YOLOv8s'\n",
    "    print(f\"âœ… Loaded YOLOv8s (fallback)\")\n",
    "\n",
    "print(f\"\\nArchitecture: {ARCH}\")\n",
    "print(f\"Run name: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa704ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Frozen backbone\n",
    "results_p1 = model.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=20,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device='cuda',\n",
    "    project=OUTPUT_DIR,\n",
    "    name=f\"{RUN_NAME}_p1\",\n",
    "    \n",
    "    # Freeze backbone\n",
    "    freeze=10,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.01,\n",
    "    lrf=0.01,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3,\n",
    "    \n",
    "    # Augmentation\n",
    "    hsv_h=0.02,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.5,\n",
    "    translate=0.15,\n",
    "    scale=0.5,\n",
    "    fliplr=0.5,\n",
    "    mosaic=1.0,\n",
    "    mixup=0.1,\n",
    "    \n",
    "    patience=0,  # No early stopping\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Phase 1 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b2d7e",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Full Fine-tune (130 epochs)\n",
    "\n",
    "Unfreeze all layers and train the full network with a lower learning rate.\n",
    "Early stopping patience=30 prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load best model from Phase 1\n",
    "p1_best = Path(OUTPUT_DIR) / f\"{RUN_NAME}_p1\" / \"weights\" / \"best.pt\"\n",
    "if not p1_best.exists():\n",
    "    p1_best = Path(OUTPUT_DIR) / f\"{RUN_NAME}_p1\" / \"weights\" / \"last.pt\"\n",
    "\n",
    "print(f\"Loading Phase 1 best: {p1_best}\")\n",
    "model2 = YOLO(str(p1_best))\n",
    "\n",
    "# Phase 2: Full fine-tune\n",
    "results_p2 = model2.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=130,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device='cuda',\n",
    "    project=OUTPUT_DIR,\n",
    "    name=f\"{RUN_NAME}_p2\",\n",
    "    \n",
    "    # No freeze â€” full network\n",
    "    freeze=0,\n",
    "    \n",
    "    # Lower LR for fine-tuning\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=0,\n",
    "    \n",
    "    # Augmentation (slightly stronger)\n",
    "    hsv_h=0.02,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.5,\n",
    "    translate=0.15,\n",
    "    scale=0.5,\n",
    "    fliplr=0.5,\n",
    "    mosaic=1.0,\n",
    "    mixup=0.15,\n",
    "    copy_paste=0.1,\n",
    "    \n",
    "    patience=30,  # Early stopping\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Phase 2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1154b52",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac14d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final best model\n",
    "final_best = Path(OUTPUT_DIR) / f\"{RUN_NAME}_p2\" / \"weights\" / \"best.pt\"\n",
    "if not final_best.exists():\n",
    "    final_best = Path(OUTPUT_DIR) / f\"{RUN_NAME}_p1\" / \"weights\" / \"best.pt\"\n",
    "\n",
    "print(f\"Evaluating: {final_best}\")\n",
    "eval_model = YOLO(str(final_best))\n",
    "\n",
    "# Test set evaluation\n",
    "metrics = eval_model.val(\n",
    "    data=DATA_YAML,\n",
    "    split='test',\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device='cuda',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Test Results:\")\n",
    "print(f\"  mAP50:     {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP50-95:  {metrics.box.map:.4f}\")\n",
    "print(f\"  Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall:    {metrics.box.mr:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ede43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some test predictions\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "test_images = sorted(glob.glob(f\"{DATASET_DIR}/images/test/pos_*.jpg\"))[:6]\n",
    "\n",
    "if test_images:\n",
    "    results = eval_model.predict(\n",
    "        source=test_images,\n",
    "        conf=0.25,\n",
    "        save=True,\n",
    "        project=OUTPUT_DIR,\n",
    "        name=f\"{RUN_NAME}_test_viz\",\n",
    "    )\n",
    "    \n",
    "    # Show results\n",
    "    viz_dir = Path(OUTPUT_DIR) / f\"{RUN_NAME}_test_viz\"\n",
    "    for img_path in sorted(viz_dir.glob('*.jpg'))[:6]:\n",
    "        display(Image(filename=str(img_path), width=640))\n",
    "        print(f\"  {img_path.name}\")\n",
    "else:\n",
    "    print(\"No positive test images found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95174110",
   "metadata": {},
   "source": [
    "## 6. Export for Deployment\n",
    "\n",
    "Export to ONNX for the Dell server deployment.  \n",
    "OpenVINO conversion should be done on the server itself for best compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "deploy_model = YOLO(str(final_best))\n",
    "\n",
    "# ONNX export\n",
    "onnx_path = deploy_model.export(format='onnx', imgsz=640, simplify=True)\n",
    "print(f\"\\nâœ… ONNX exported: {onnx_path}\")\n",
    "\n",
    "# Show model size\n",
    "import os\n",
    "pt_size = os.path.getsize(str(final_best)) / 1e6\n",
    "onnx_size = os.path.getsize(str(onnx_path)) / 1e6 if onnx_path else 0\n",
    "print(f\"\\nModel sizes:\")\n",
    "print(f\"  PyTorch (.pt):  {pt_size:.1f} MB\")\n",
    "print(f\"  ONNX (.onnx):   {onnx_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65700984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try OpenVINO export (may fail on Colab â€” that's OK, do it on the Dell server)\n",
    "try:\n",
    "    !pip install -q openvino openvino-dev\n",
    "    ov_path = deploy_model.export(format='openvino', imgsz=640, half=True)\n",
    "    print(f\"âœ… OpenVINO FP16 exported: {ov_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ OpenVINO export failed on Colab (expected): {e}\")\n",
    "    print(\"Export on Dell server instead:\")\n",
    "    print(\"  yolo export model=best.pt format=openvino half=True imgsz=640\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e06ae",
   "metadata": {},
   "source": [
    "## 7. Download Trained Model\n",
    "\n",
    "Download `best.pt` (and optionally ONNX) to your local machine,\n",
    "then copy to Dell server:\n",
    "\n",
    "```bash\n",
    "scp best.pt rndpig@192.168.7.215:/home/rndpig/deer-deterrent/dell-deployment/models/best.pt\n",
    "ssh rndpig@192.168.7.215 \"cd /home/rndpig/deer-deterrent && docker compose restart ml-detector\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac372ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Copy best model to a convenient location\n",
    "download_dir = Path('/content/download')\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy best.pt\n",
    "shutil.copy2(str(final_best), download_dir / 'best.pt')\n",
    "print(f\"Copied best.pt ({final_best.stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "# Copy ONNX if available\n",
    "onnx_file = final_best.with_suffix('.onnx')\n",
    "if onnx_file.exists():\n",
    "    shutil.copy2(str(onnx_file), download_dir / 'best.onnx')\n",
    "    print(f\"Copied best.onnx ({onnx_file.stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "# Save training summary\n",
    "summary = {\n",
    "    'architecture': ARCH,\n",
    "    'dataset_version': '2.0',\n",
    "    'test_map50': float(metrics.box.map50),\n",
    "    'test_map50_95': float(metrics.box.map),\n",
    "    'test_precision': float(metrics.box.mp),\n",
    "    'test_recall': float(metrics.box.mr),\n",
    "    'model_size_mb': final_best.stat().st_size / 1e6,\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "with open(download_dir / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Download\n",
    "print(\"\\nðŸ“¥ Downloading files...\")\n",
    "files.download(str(download_dir / 'best.pt'))\n",
    "files.download(str(download_dir / 'training_summary.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170854c",
   "metadata": {},
   "source": [
    "## 8. Deployment Instructions\n",
    "\n",
    "After downloading `best.pt`:\n",
    "\n",
    "```bash\n",
    "# 1. Copy model to Dell server\n",
    "scp best.pt rndpig@192.168.7.215:/home/rndpig/deer-deterrent/dell-deployment/models/best.pt\n",
    "\n",
    "# 2. SSH into server\n",
    "ssh rndpig@192.168.7.215\n",
    "cd /home/rndpig/deer-deterrent\n",
    "\n",
    "# 3. Update ml-detector Dockerfile to use ultralytics>=8.4.0\n",
    "# (required for YOLO26s inference)\n",
    "\n",
    "# 4. Add CLAHE preprocessing to ml-detector inference\n",
    "# (must match training preprocessing)\n",
    "\n",
    "# 5. Rebuild and restart\n",
    "docker compose build ml-detector\n",
    "docker compose up -d --force-recreate ml-detector\n",
    "\n",
    "# 6. Test with a known deer image\n",
    "curl -X POST http://localhost:8001/detect -F 'file=@test_deer.jpg'\n",
    "```\n",
    "\n",
    "### Important: CLAHE must be applied during inference too!\n",
    "The model was trained on CLAHE-enhanced images. Add this to the ml-detector's `/detect` endpoint:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def enhance_ir_image(image: np.ndarray) -> np.ndarray:\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced_l = clahe.apply(l)\n",
    "    return cv2.cvtColor(cv2.merge([enhanced_l, a, b]), cv2.COLOR_LAB2BGR)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
